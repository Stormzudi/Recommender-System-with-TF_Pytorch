## 1、Spark SQL 概述

**Spark SQL概念**

* Spark SQL is Apache Spark's module for working with structured data.
  * 它是spark中用于处理结构化数据的一个模块

**Spark SQL历史**

- Hive是目前大数据领域，事实上的数据仓库标准。

![s9](pics/s9.png)

- Shark：shark底层使用spark的基于内存的计算模型，从而让性能比Hive提升了数倍到上百倍。
- 底层很多东西还是依赖于Hive，修改了内存管理、物理计划、执行三个模块
- 2014年6月1日的时候，Spark宣布了不再开发Shark，全面转向Spark SQL的开发

**Spark SQL优势**

- Write Less Code

![s10](pics/s10.png)

* Performance

![s11](pics/s11.png)

python操作RDD，转换为可执行代码，运行在java虚拟机，涉及两个不同语言引擎之间的切换，进行进程间		通信很耗费性能。

DataFrame

- 是RDD为基础的分布式数据集，类似于传统关系型数据库的二维表，dataframe记录了对应列的名称和类型
- dataFrame引入schema和off-heap(使用操作系统层面上的内存)
  - 1、解决了RDD的缺点
  - 序列化和反序列化开销大
  - 频繁的创建和销毁对象造成大量的GC
  - 2、丢失了RDD的优点
  - RDD编译时进行类型检查
  - RDD具有面向对象编程的特性



用scala/python编写的RDD比Spark SQL编写转换的RDD慢，涉及到执行计划

- CatalystOptimizer：Catalyst优化器
- ProjectTungsten：钨丝计划，为了提高RDD的效率而制定的计划
- Code gen：代码生成器

![s12](pics/s12.png)

直接编写RDD也可以自实现优化代码，但是远不及SparkSQL前面的优化操作后转换的RDD效率高，快1倍左右

优化引擎：类似mysql等关系型数据库基于成本的优化器

首先执行逻辑执行计划，然后转换为物理执行计划(选择成本最小的)，通过Code Generation最终生成为RDD

* Language-independent API

  用任何语言编写生成的RDD都一样，而使用spark-core编写的RDD，不同的语言生成不同的RDD

- Schema

  结构化数据，可以直接看出数据的详情

  在RDD中无法看出，解释性不强，无法告诉引擎信息，没法详细优化。

**为什么要学习sparksql **

sparksql特性

  *  1、易整合
  *  2、统一的数据源访问
  *  3、兼容hive
  *  4、提供了标准的数据库连接（jdbc/odbc）
